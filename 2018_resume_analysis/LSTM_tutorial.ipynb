{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 한글설명\n",
    "http://www.hanbit.co.kr/channel/category/category_view.html?cms_code=CMS6074576268\n",
    "\n",
    "* 깃허브\n",
    "https://github.com/GarrettHoffman/lstm-oreilly/blob/master/Stock%20Market%20Sentiment%20with%20LSTMs%20and%20TensorFlow.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load utils.py\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_ST_message(text):\n",
    "    \"\"\"\n",
    "    Preprocesses raw message data for analysis\n",
    "    :param text: String. ST Message\n",
    "    :return: List of Strings.  List of processed text tokes\n",
    "    \"\"\"\n",
    "    # Define ST Regex Patters\n",
    "    REGEX_PRICE_SIGN = re.compile(r'\\$(?!\\d*\\.?\\d+%)\\d*\\.?\\d+|(?!\\d*\\.?\\d+%)\\d*\\.?\\d+\\$')\n",
    "    REGEX_PRICE_NOSIGN = re.compile(r'(?!\\d*\\.?\\d+%)(?!\\d*\\.?\\d+k)\\d*\\.?\\d+')\n",
    "    REGEX_TICKER = re.compile('\\$[a-zA-Z]+')\n",
    "    REGEX_USER = re.compile('\\@\\w+')\n",
    "    REGEX_LINK = re.compile('https?:\\/\\/[^\\s]+')\n",
    "    REGEX_HTML_ENTITY = re.compile('\\&\\w+')\n",
    "    REGEX_NON_ACSII = re.compile('[^\\x00-\\x7f]')\n",
    "    REGEX_PUNCTUATION = re.compile('[%s]' % re.escape(string.punctuation.replace('<', '')).replace('>', ''))\n",
    "    REGEX_NUMBER = re.compile(r'[-+]?[0-9]+')\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace ST \"entitites\" with a unique token\n",
    "    text = re.sub(REGEX_TICKER, ' <TICKER> ', text)\n",
    "    text = re.sub(REGEX_USER, ' <USER> ', text)\n",
    "    text = re.sub(REGEX_LINK, ' <LINK> ', text)\n",
    "    text = re.sub(REGEX_PRICE_SIGN, ' <PRICE> ', text)\n",
    "    text = re.sub(REGEX_PRICE_NOSIGN, ' <NUMBER> ', text)\n",
    "    text = re.sub(REGEX_NUMBER, ' <NUMBER> ', text)\n",
    "    # Remove extraneous text data\n",
    "    text = re.sub(REGEX_HTML_ENTITY, \"\", text)\n",
    "    text = re.sub(REGEX_NON_ACSII, \"\", text)\n",
    "    text = re.sub(REGEX_PUNCTUATION, \"\", text)\n",
    "    # Tokenize and remove < and > that are not in special tokens\n",
    "    words = \" \".join(token.replace(\"<\", \"\").replace(\">\", \"\")\n",
    "                     if token not in ['<TICKER>', '<USER>', '<LINK>', '<PRICE>', '<NUMBER>']\n",
    "                     else token\n",
    "                     for token\n",
    "                     in text.split())\n",
    "\n",
    "    return words\n",
    "\n",
    "def create_lookup_tables(words):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param words: Input list of words\n",
    "    :return: A tuple of dicts.  The first dict maps a vocab word to and integeter\n",
    "             The second maps an integer back to to the vocab word\n",
    "    \"\"\"\n",
    "    word_counts = Counter(words)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab, 1)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "def encode_ST_messages(messages, vocab_to_int):\n",
    "    \"\"\"\n",
    "    Encode ST Sentiment Labels\n",
    "    :param messages: list of list of strings. List of message tokens\n",
    "    :param vocab_to_int: mapping of vocab to idx\n",
    "    :return: list of ints. Lists of encoded messages\n",
    "    \"\"\"\n",
    "    messages_encoded = []\n",
    "    for message in messages:\n",
    "        messages_encoded.append([vocab_to_int[word] for word in message.split()])\n",
    "\n",
    "    return np.array(messages_encoded)\n",
    "\n",
    "def encode_ST_labels(labels):\n",
    "    \"\"\"\n",
    "    Encode ST Sentiment Labels\n",
    "    :param labels: Input list of labels\n",
    "    :return: numpy array.  The encoded labels\n",
    "    \"\"\"\n",
    "    return np.array([1 if sentiment == 'bullish' else 0 for sentiment in labels])\n",
    "\n",
    "def drop_empty_messages(messages, labels):\n",
    "    \"\"\"\n",
    "    Drop messages that are left empty after preprocessing\n",
    "    :param messages: list of encoded messages\n",
    "    :return: tuple of arrays. First array is non-empty messages, second array is non-empty labels\n",
    "    \"\"\"\n",
    "    non_zero_idx = [ii for ii, message in enumerate(messages) if len(message) != 0]\n",
    "    messages_non_zero = np.array([messages[ii] for ii in non_zero_idx])\n",
    "    labels_non_zero = np.array([labels[ii] for ii in non_zero_idx])\n",
    "    return messages_non_zero, labels_non_zero\n",
    "\n",
    "def zero_pad_messages(messages, seq_len):\n",
    "    \"\"\"\n",
    "    Zero Pad input messages\n",
    "    :param messages: Input list of encoded messages\n",
    "    :param seq_ken: Input int, maximum sequence input length\n",
    "    :return: numpy array.  The encoded labels\n",
    "    \"\"\"\n",
    "    messages_padded = np.zeros((len(messages), seq_len), dtype=int)\n",
    "    for i, row in enumerate(messages):\n",
    "        messages_padded[i, -len(row):] = np.array(row)[:seq_len]\n",
    "\n",
    "    return np.array(messages_padded)\n",
    "\n",
    "def train_val_test_split(messages, labels, split_frac, random_seed=None):\n",
    "    \"\"\"\n",
    "    Zero Pad input messages\n",
    "    :param messages: Input list of encoded messages\n",
    "    :param labels: Input list of encoded labels\n",
    "    :param split_frac: Input float, training split percentage\n",
    "    :return: tuple of arrays train_x, val_x, test_x, train_y, val_y, test_y\n",
    "    \"\"\"\n",
    "    # make sure that number of messages and labels allign\n",
    "    assert len(messages) == len(labels)\n",
    "    # random shuffle data\n",
    "    if random_seed:\n",
    "        np.random.seed(random_seed)\n",
    "    shuf_idx = np.random.permutation(len(messages))\n",
    "    messages_shuf = np.array(messages)[shuf_idx] \n",
    "    labels_shuf = np.array(labels)[shuf_idx]\n",
    "\n",
    "    #make splits\n",
    "    split_idx = int(len(messages_shuf)*split_frac)\n",
    "    train_x, val_x = messages_shuf[:split_idx], messages_shuf[split_idx:]\n",
    "    train_y, val_y = labels_shuf[:split_idx], labels_shuf[split_idx:]\n",
    "\n",
    "    test_idx = int(len(val_x)*0.5)\n",
    "    val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "    val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "    return train_x, val_x, test_x, train_y, val_y, test_y\n",
    "    \n",
    "def get_batches(x, y, batch_size=100):\n",
    "    \"\"\"\n",
    "    Batch Generator for Training\n",
    "    :param x: Input array of x data\n",
    "    :param y: Input array of y data\n",
    "    :param batch_size: Input int, size of batch\n",
    "    :return: generator that returns a tuple of our x batch and y batch\n",
    "    \"\"\"\n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 가공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read data from csv file\n",
    "data = pd.read_csv(\"../data/StockTwits_SPY_Sentiment_2017.gz\",\n",
    "                   encoding=\"utf-8\",\n",
    "                   compression=\"gzip\",\n",
    "                   index_col=0)\n",
    "\n",
    "# get messages and sentiment labels\n",
    "messages = data.message.values\n",
    "labels = data.sentiment.values\n",
    "\n",
    "# View sample of messages with sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$SPY crazy day so far!</td>\n",
       "      <td>bearish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$SPY Will make a new ATH this week. Watch it!</td>\n",
       "      <td>bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$SPY $DJIA white elephant in room is $AAPL. Up...</td>\n",
       "      <td>bearish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$SPY blocks above. We break above them We shou...</td>\n",
       "      <td>bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$SPY Nothing happening in the market today, gu...</td>\n",
       "      <td>bearish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message sentiment\n",
       "0                             $SPY crazy day so far!   bearish\n",
       "1      $SPY Will make a new ATH this week. Watch it!   bullish\n",
       "2  $SPY $DJIA white elephant in room is $AAPL. Up...   bearish\n",
       "3  $SPY blocks above. We break above them We shou...   bullish\n",
       "4  $SPY Nothing happening in the market today, gu...   bearish"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 전처리\n",
    "messages = np.array([preprocess_ST_message(message) for message in messages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<TICKER> crazy day so far',\n",
       "       '<TICKER> will make a new ath this week watch it',\n",
       "       '<TICKER> <TICKER> white elephant in room is <TICKER> up <NUMBER> since election strong headwinds wtrump trade strong dollar how many <NUMBER> s do you see',\n",
       "       '<TICKER> blocks above we break above them we should push to double top',\n",
       "       '<TICKER> nothing happening in the market today guess ill go to the store and spend some'],\n",
       "      dtype='<U244')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_lexicon = \" \".join(messages).split()\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(full_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length messages: 1\n",
      "Maximum message length: 244\n",
      "Average message length: 78.21856920395598\n"
     ]
    }
   ],
   "source": [
    "messages_lens = Counter([len(x) for x in messages])\n",
    "print(\"Zero-length messages: {}\".format(messages_lens[0]))\n",
    "print(\"Maximum message length: {}\".format(max(messages_lens)))\n",
    "print(\"Average message length: {}\".format(np.mean([len(x) for x in messages])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 빈 메시지 버리기\n",
    "messages, labels = drop_empty_messages(messages, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages = encode_ST_messages(messages, vocab_to_int)\n",
    "labels = encode_ST_labels(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모든 메시지를 같은 길이로 바꿔주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages = zero_pad_messages(messages, seq_len=244)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   38,   50,  382],\n",
       "       [   0,    0,    0, ...,   61,  198,   15],\n",
       "       [   0,    0,    0, ...,  110,   20,   59],\n",
       "       [   0,    0,    0, ...,    4,  456,  146],\n",
       "       [   0,    0,    0, ...,    8, 1454,  100]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train valid test 셋 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Set Size\n",
      "Train set: \t\t(77572, 244) \n",
      "Validation set: \t(9697, 244) \n",
      "Test set: \t\t(9697, 244)\n"
     ]
    }
   ],
   "source": [
    "train_x, val_x, test_x, train_y, val_y, test_y = train_val_test_split(messages, labels, split_frac=0.80)\n",
    "\n",
    "print(\"Data Set Size\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네트워크 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    \"\"\"\n",
    "    Create the model inputs\n",
    "    \"\"\"\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    keep_prob_ = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs_, labels_, keep_prob_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embedding은 Word를 R차원의 Vector로 매핑시켜주는 것을 말한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_embedding_layer(inputs_, vocab_size, embed_size):\n",
    "    \"\"\"\n",
    "    Create the embedding layer\n",
    "    \"\"\"\n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "    \n",
    "    return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM layers and wrap them in a Dropout Layer. Dropout is a regularization technique <br>\n",
    " we stack these layers using a MultiRNNCell, generate a zero initial state and connect our stacked LSTM layer to our word embedding inputs using dynamic_rnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lstm_layers(lstm_sizes, embed, keep_prob_, batch_size):\n",
    "    \"\"\"\n",
    "    Create the LSTM layers\n",
    "    \"\"\"\n",
    "    lstms = [tf.contrib.rnn.BasicLSTMCell(size) for size in lstm_sizes]\n",
    "    # Add dropout to the cell\n",
    "    drops = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_) for lstm in lstms]\n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(drops)\n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    lstm_outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)\n",
    "    \n",
    "    return initial_state, lstm_outputs, cell, final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로, 우리 모델의 loss function, 최적화 방법, 그리고 정확도를 정의하기 위한 함수를 생성한다. loss와 정확도가 단순히 결과 값에서 계산하는 것에 불과하더라도, TensorFlow에서는 모든 과정이 계산 그래프의 일부이다. 그렇기 때문에, 우리는 loss, 최적화 방법, 그리고 정확도 계산 노드를 그래프의 관점에서 정의할 필요가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cost_fn_and_opt(lstm_outputs, labels_, learning_rate):\n",
    "    \"\"\"\n",
    "    Create the Loss function and Optimizer\n",
    "    \"\"\"\n",
    "    predictions = tf.contrib.layers.fully_connected(lstm_outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "    loss = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    optimzer = tf.train.AdadeltaOptimizer (learning_rate).minimize(loss)\n",
    "\n",
    "def build_accuracy(predictions, labels_):\n",
    "\n",
    "    \"\"\"\n",
    "    Create accuracy\n",
    "\n",
    "    \"\"\"\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_and_train_network(lstm_sizes, vocab_size, embed_size, epochs, batch_size,\n",
    "                            learning_rate, keep_prob, train_x, val_x, train_y, val_y):\n",
    "    \n",
    "    inputs_, labels_, keep_prob_ = model_inputs()\n",
    "    embed = build_embedding_layer(inputs_, vocab_size, embed_size)\n",
    "    initial_state, lstm_outputs, lstm_cell, final_state = build_lstm_layers(lstm_sizes, embed, keep_prob_, batch_size)\n",
    "    predictions, loss, optimizer = build_cost_fn_and_opt(lstm_outputs, labels_, learning_rate)\n",
    "    accuracy = build_accuracy(predictions, labels_)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        n_batches = len(train_x)//batch_size\n",
    "        for e in range(epochs):\n",
    "            state = sess.run(initial_state)\n",
    "            \n",
    "            train_acc = []\n",
    "            for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "                feed = {inputs_: x,\n",
    "                        labels_: y[:, None],\n",
    "                        keep_prob_: keep_prob,\n",
    "                        initial_state: state}\n",
    "                loss_, state, _,  batch_acc = sess.run([loss, final_state, optimizer, accuracy], feed_dict=feed)\n",
    "                train_acc.append(batch_acc)\n",
    "                \n",
    "                if (ii + 1) % n_batches == 0:\n",
    "                    \n",
    "                    val_acc = []\n",
    "                    val_state = sess.run(lstm_cell.zero_state(batch_size, tf.float32))\n",
    "                    for xx, yy in get_batches(val_x, val_y, batch_size):\n",
    "                        feed = {inputs_: xx,\n",
    "                                labels_: yy[:, None],\n",
    "                                keep_prob_: 1,\n",
    "                                initial_state: val_state}\n",
    "                        val_batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                        val_acc.append(val_batch_acc)\n",
    "                    \n",
    "                    print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                          \"Batch: {}/{}...\".format(ii+1, n_batches),\n",
    "                          \"Train Loss: {:.3f}...\".format(loss_),\n",
    "                          \"Train Accruacy: {:.3f}...\".format(np.mean(train_acc)),\n",
    "                          \"Val Accuracy: {:.3f}\".format(np.mean(val_acc)))\n",
    "    \n",
    "        saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameter 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Inputs and Hyperparameters\n",
    "lstm_sizes = [128, 64]\n",
    "vocab_size = len(vocab_to_int) + 1 #add one for padding\n",
    "embed_size = 300\n",
    "epochs = 50\n",
    "batch_size = 256\n",
    "learning_rate = 0.1\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-5b1f221a9ea4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     build_and_train_network(lstm_sizes, vocab_size, embed_size, epochs, batch_size,\n\u001b[1;32m----> 3\u001b[1;33m                             learning_rate, keep_prob, train_x, val_x, train_y, val_y)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-8d4ea95271c5>\u001b[0m in \u001b[0;36mbuild_and_train_network\u001b[1;34m(lstm_sizes, vocab_size, embed_size, epochs, batch_size, learning_rate, keep_prob, train_x, val_x, train_y, val_y)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0membed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_embedding_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstm_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstm_cell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_lstm_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_cost_fn_and_opt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    build_and_train_network(lstm_sizes, vocab_size, embed_size, epochs, batch_size,\n",
    "                            learning_rate, keep_prob, train_x, val_x, train_y, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
